---
title: "Chapter 13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.




# Chapter 13. Nonlinear Classification Models

## 13.1 Nonlinear discriminant analysis 

### Quadratic and regularized discriminant analysis (QDA & RDA)

### Mixture discriminant analysis (MDA)

## 13.2 Neural networks

## 13.3 Flexible discriminant analysis 

## 13.4 Support vector machines 

## 13.5 K-Nearest neighbors 

## 13.6 Naive Bayes


## 13.7 Computing 

```{r}
library(caret)
library(earth)
library(kernlab)
library(klaR)
library(MASS)
library(mda)
library(nnet)
library(rrcov)
```


### Nonlinear discriminant analysis

```{r}
mdaModel <- mda(Class ~.,
                # Reduce the data to the relevant predictors and the class variable
                data = training[pre2008, c("Class", reducedSet)],
                subclasses = 3)
mdaModel
```

```{r}
predict(mdaModel, newdata = head(training[-pre2008, reducedSet]))
```

```{r}
ctrl <- trainControl(summaryFunction = twoClassSummary, classProbs = TRUE)

set.seed(476)
mdaFit <- train(training[,reducedSet], trainingSet$Class, method = "mda", metric = "ROC",
                tuneGrid <- expand.grid(.subclasses = 1:8),
                trControl = ctrl)
```


### Neural networks

```{r}
head(class.ind(training$Class))

set.seed(800)

nnetMod <- nnet(Class ~ NumCI + CI.1960, data = training[pre2008,], size = 3, decay = 0.1)

nnetMod

predict(nnetMod, newdata = head(testing))

predict(nnetMod, newdata = head(testing), type = "class")
```

```{r}
nnetGrid <- expand.grid(.size = c(1:10), .decay = c(0, 0.1, 1, 2))

maxSize = max(nnetGrid$.size)

numWts <- 1 * (maxSize * (length(reducedSet) + 1) + maxSize + 1)

set.seed(476)

nnetFit <- train(x = training[,reducedSet],
                 y = training$Class, 
                 method = "nnet",
                 metric = "ROC",
                 preProc = c("center", "scale", "spatialSign"),
                 tuneGrid = nnetGrid,
                 trace = FALSE,
                 maxit = 2000,
                 MaxNWts = numWts,
                 trControl = ctrl)
nnetFit
```


### Flexible discriminant analysis 

```{r}
library(mda)
library(earth)

fdaModel <- fda(Class ~ Day + NumCI, data = training[pre2008,], method = earth)

summary(fdaModel$fit)

predict(fdaModel, head(training[-pre2008,]))
```


### Support vector machines

```{r}
set.seed(202)
sigmaRangeReduced <- sigest(as.matrix(training[,reducedSet]))
svmRGridReduced <- expand.grid(.sigma = sigmaRangeReduced[1],
                               .C = 2^(seq(-4, 4)))
```

```{r}
set.seed(476)
svmRModel <- train(training[,reducedSet], training$Class, method = "svmRadial", metric = "ROC",
                   preProc = c("center", "scale"), tuneGrid = svmRGridReduced, fit = FALSE, trControl = ctrl)
svmRModel
```

```{r}
library(kernlab)
predict(svmRModel, newdata = head(training[-pre2008, reducedSet]))
predict(svmRMode, newdata = head(training[-pre2008, reducedSet]), type = "prob")
```


### K-nearest neighbors 

```{r}
set.seed(476)
knnFit <- train(training[,reducedSet], training$Class, method = "knn", metric = "ROC", 
                preProc = c("center", "scale"), 
                tuneGrid = data.frame(.k = c(4*(0:5)+1, 20*(1:5)+1, 50*(2:9)+1)),
                trControl = ctrl)

knnFit$pred <- merge(knnFit$pred, knnFit$bestTune)
knnRoc <- roc(response = knnFit$pred$obs, predictor = knnFit$pred$successful, levels = rev(levels(knnFit$pred$obs)))

plot(knnRoc, legacy.axes = TRUE)
```


### Naive bayes

```{r}
# Some predictors are already stored as factors 
factors <- c("SponsorCode", "ContractValueBand", "Month", "Weekday")

# Get the other predictors from the reduced set
nbPredictors <- factorPredictors[factorPredictors %in% reducedSet]
nbPredictors <- c(nbPredictors, factors)

# Leek only those that are needed
nbTraining <- training[,c("Class", nbPredictors)]
nbTesting <- testing[,c("Class", nbPredictors)]

# Loop through the predictors and convert some to factors

for(i in nbPredictors) {
  varLevels <- sort(unique(training[,i]))
  if(length(varLevels)<=15{
    nbTraining[,i] <- factor(nbTraining[,i], levels = paste(varLevels))
    nbTesting[,i] <- factor(nbTesting[,i], levels = paste(varLevels))
  })
}
```

```{r}
library(klaR)
nBayesFit <- NaiveBayes(Class ~., data = nbTraining[pre2008,], 
                        # should the non-parametric estimate be used
                        usekernel = TRUE,
                        # Laplace correction value 
                        fL = 2)
predict(nBayesFit, newdata = head(nbTesting))
```





















