---
title: "Chapter 12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.




# Chapter 12. Discriminant Analysis and Other Linear Classification Models

## 12.1 Case study: predicting successful grant applications 

## 12.2 Logistic regression

## 12.3 Linear discriminant analysis 

## 12.4 Partial least squares discriminant analysis 

## 12.5 Penalized models 

## 12.6 Nearest shrunken centroids


## 12.7 Computing 

```{r}
library(AppliedPredictiveModeling)
library(caret)
library(glmnet)
library(MASS)
library(pamr)
library(pls)
library(pROC)
library(rms)
library(sparseLDA)
library(subselect)
library(tibble)
library(tidyverse)
```

```{r}
rm(list=ls())
```


```{r}
data()
```

```{r}
training <- read.csv("unimelb_training.csv")
testing <- read.csv("unimelb_test.csv")
```


```{r}
.libPaths()
```

```{r}
length(fullSet)
head(fullSet)
length(reducedSet)
head(reducedSet)
```

```{r}
reducedCovMat <- cov(training[,reducedSet])
trimmingResults <- trim.matrix(reducedCovMat)
names(trimmingResults)
trimmingResult$names.discarded

fullCovMat <- cov(training[,fullSet])
fullSetResults <- trim.matrix(fullCovMat)

fullSetResults$names.discard

```


```{r}
ctrl <- trainControl(summaryFunction = twoClassSummary, classProbs = TRUE)

ctrl <-trainControl(method = "LGOCV",
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE,
                    index = list(TrainSet = pre2008))

ctrl <-trainControl(method = "LGOCV",
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE,
                    index = list(TrainSet = pre2008),
                    savePredictions = TRUE)

```



### logistic regression

```{r}
levels(training$Class)

modelFit <- glm(Class ~ Day, data = training[pre2008,],
                # family relates to the distribution of the data. A value of binomial is used for logistic regression
                family = binomial)

modelFit
```

```{r}
successProb <- 1 - predict(modelFit, newdata = data.frame(Day=c(10, 150, 300, 350)),
                           type = "response")
successProb
```

```{r}
daySquaredModel <- glm(Class ~ Day + I(Day^2), data = training[pre2008,], family = binomial)
daySquaredModel
```

```{r}
rcsFit <- lrm(Class ~ rcs(Day), data = training[pre2008,])
rcsFit
```

```{r}
dayProfile <- Predict(rcsFit,
                      # Specify the range of the plot variable 
                      Day = 0:365,
                      # Flip the prediction to get the model for successful grants 
                      fun = function(x)-x)
plot(dayProfile, ylab = "Log Odds")
```

```{r}
training$Day2 <- training$Day^2
fullSet <- c(fullSet, "Day2")
reducedSet <- c(reducedSet, "Day2")
```

```{r}
set.seed(476)
lrFull <- train(training[,fullSet],
                y = training$Class,
                method = "glm",
                metric = "ROC",
                trControl = ctrl)
lrFull
```

```{r}
set.seed(476)
lrReduced <- train(training[,reducedSet],
                y = training$Class,
                method = "glm",
                metric = "ROC",
                trControl = ctrl)
lrReduced
```

```{head(lrReduced$predr}
head(lrReduced$pred)
```

```{r}
confusionMatrix(data = lrReduced$pred$pred, reference = lrReduced$pred$obs)
```

```{r}
reducedRoc <- roc(response = lrReduced$pred$obs, predictor = lrReduced$pred$successful,
                  levels = rev(levels(lrReduced$pred$obs)))
plot(reducedRoc, legacy.axes = TRUE)
auc(reducedRoc)
```


### linear discriminant analysis 

```{r}
grantPreProcess <- preProcess(training[pre2008, reducedSet])
grantPreProcess

scaled2008 <- predict(grantPreProcess, newdata = training[pre2008, reducedSet])
scaled2008HoldOut <- predict(grantPreProcess, newdata = training[-pre2008, reducedSet])

ldaModel <- lda(x = scaledPre2008, grouping = training$Class[pre2008])

head(ldaModel$scaling)

ldaHoldOutPredictions <- predict(ldaMode, scaled2008HoldOut)

```

```{r}
set.seed(476)
ldaFit1 <- train(x = training[,reducedSet],
                 y = training$Class,
                 method = "lda",
                 preProc = c("center","scale"),
                 metric = "ROC",
                 trControl = ctrl)
ldaFit1
```

```{r}
ldaTestClasses <- predict(ldaFit1, newdata = testing[,reducedSet])
ldaTestProbs <- predict(ldaFit, newdata = testing[,reducedSet], type = "prob")
```


### partial least squares discriminant analysis

```{r}
plsdaModel <- plsda(x = training[pre2008, reducedSet],
                    y = training[pre2008, "Class"],
                    # The data should be on the same scale for PLS. The scale option applies this step
                    scale = TRUE,
                    # Use Bayes method to compute the probabilities
                    probMethod = "Bayes",
                    # Specify the number of components to model
                    ncom = 4)

plsPred <- predict(plsdaModel, newdata = training[-pre2008, reducedSet])

head(plsPred)

plsProbs <- predict(plsdaModel, newdata = training[-pre2008, reducedSet], type = "prob")

head(plsProbs)
```

```{r}
set.seed(476)
plsFit2 <- train(x = training[, reducedSet],
                 y = training$Class,
                 method = "pls",
                 tuneGrid = expand.grid(.ncomp = 1:10),
                 preProc = c("center","scale"),
                 metric = "ROC",
                 trControl = ctrl)

plsImpGrant <- varImp(plsFit2, scale = FALSE)
plsImpGrant
plot(plsImpGrant, top = 20, scales = list(y = list(cex = .95)))
```



### penalized models 

```{r}
library(glmnet)

glmnetModel <- glmnet(x = as.matrix(training[,fullSet]),
                      y = training$Class,
                      family = "binomial")

predict(glmnetModel, 
        newx = as.matrix(training[1:5, fullSet]), 
        s = c(0.05, 0.1, 0.2),
        type = "class")

predict(glmnetModel, 
        newx = as.matrix(training[1:5, fullSet]), 
        s = c(0.05, 0.1, 0.2),
        type = "nonzero")
```

```{r}
glmnGrid <- expand.grid(.alpha = c(0, .1, .2, .4, .6, .8, 1),
                        .lambda = seq(.01, .2, length = 40))

set.seed(476)

glmnTuned <- train(training[,fullSet],
                   y = training$Class,
                   method = "glmnet",
                   tuneGrid = glmnGrid,
                   preProc = c("center", "scale"),
                   metric = "ROC",
                   trControl = ctrl)

plot(glmnTuned, plotType = "level")
```

```{r}
library(sparseLDA)

sparseLdaModel <- sda(x = as.matrix(training[,fullSet]),
                      y = training$Class,
                      lambda = 0.01,
                      stop = -6)

```


### nearest shrunken centroids

```{r}
# switch dimensions using the t() function to transpose the data 
# This also implicitly converts the training data frame to a matrix
inputData <- list(x = t(training[,fullSet]),y = training$Class)

library(pamr)
nscModel <- pamr.train(data = inputData)

exampleData <- t(training[1:5,fullSet])
pamr.predict(nscModel, newx = exampleData, threshold = 5)

thresh17Vars <- pamr.predict(nscModel, newx = exampleData, threshold = 17, type = "nonzero")
```


```{r}
nscGrid <- data.frame(.threshold = 0:25)
set.seed(476)
nscTuned <- train(x = training[,fullSet],
                  y = training$Class,
                  method = "pam",
                  preProc = c("center","scale"),
                  tuneGrid = nscGrid,
                  metric = "ROC",
                  trControl = ctrl)
```


```{r}
predictors(nscTuned)

varImp(nscTuned, scale = TRUE)
```


















